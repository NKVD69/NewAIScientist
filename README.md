# AI Co-Scientist: Multi-Agent System for Scientific Discovery

Une impl√©mentation du syst√®me **AI co-scientist**, inspir√©e par les travaux de **Sakana.ai** ("The AI Scientist") et le papier de **Google DeepMind** "Towards an AI co-scientist" (2025).

## üéØ Vue d'ensemble

Ce syst√®me est une architecture multi-agent con√ßue pour :
- **Rechercher** et analyser la litt√©rature scientifique existante (RAG avec ArXiv).
- **G√©n√©rer** des hypoth√®ses scientifiques novelles et fond√©es ("grounded").
- **√âvaluer** la qualit√©, la nouveaut√© et la testabilit√©.
- **D√©battre** et **classer** les hypoth√®ses via un tournoi (syst√®me Elo).
- **√âvoluer** et **am√©liorer** les hypoth√®ses it√©rativement.
- **Synth√©tiser** les insights et fournir une vue d'ensemble de la recherche.

## üèóÔ∏è Architecture

### Agents Sp√©cialis√©s

#### 1. **Literature Agent (Nouveau)**
- Interroge l'API **ArXiv** pour trouver des papiers pertinents en Open Access.
- Analyse les r√©sum√©s pour fournir un contexte scientifique r√©el au syst√®me.

#### 2. **Generation Agent** 
- **Mode RAG** : Utilise le contexte bibliographique fourni par le Literature Agent pour g√©n√©rer des hypoth√®ses ancr√©es dans la r√©alit√©.
- **Mode LLM** : Appelle un LLM local (Ollama, LM Studio) pour la cr√©ativit√©.

#### 3. **Reflection Agent**
- Agit comme un "reviewer scientifique senior". Analyse l'hypoth√®se et retourne une critique d√©taill√©e ainsi que des scores pr√©cis (Correctness, Novelty, Testability, Quality).

#### 4. **Ranking Agent**
- Classe les hypoth√®ses via un **tournoi Elo** en simulant des d√©bats scientifiques.

#### 5. **Proximity Agent**
- Calcule la **similarit√©** entre les hypoth√®ses pour le clustering et la d√©duplication.

#### 6. **Evolution Agent**
- Am√©liore les hypoth√®ses via des strat√©gies comme l'enrichissement, la simplification ou la combinaison.

#### 7. **Meta-Review Agent**
- Synth√©tise les r√©sultats, identifie les tendances et g√©n√®re un aper√ßu de la recherche.

#### 8. **Supervisor Agent**
- Orchestre tous les agents et g√®re une file de t√¢ches asynchrone.

## üöÄ Installation

```bash
# 1. Cloner le d√©p√¥t
git clone https://github.com/your-repo/ai-co-scientist.git
cd ai-co-scientist

# 2. Installer les d√©pendances
pip install -r requirements.txt

# 3. (Optionnel) Configurer un LLM local (voir section ci-dessous)
```

## üñ•Ô∏è Interface Graphique (GUI)

Une interface moderne bas√©e sur **Streamlit** est disponible pour piloter l'assistant sans toucher au code.

### Lancer l'interface

```bash
streamlit run app.py
```

### Fonctionnalit√©s
- **Configuration** : Activez/D√©sactivez le LLM et configurez l'URL (Ollama/LM Studio) directement depuis la barre lat√©rale.
- **Tableau de Bord** : Suivez la g√©n√©ration, la revue et les tournois en temps r√©el.
- **Litt√©rature** : Visualisez les papiers ArXiv r√©cup√©r√©s et utilis√©s pour la g√©n√©ration.
- **Visualisation** : Graphiques interactifs des scores Elo et de la distribution Qualit√©/Nouveaut√©.
- **Exploration** : Inspectez chaque hypoth√®se, ses critiques et ses preuves.
- **Export** : T√©l√©chargez le rapport complet en JSON.

## üß† Connexion √† un LLM Local (Ollama/LM Studio)

Le syst√®me est con√ßu pour fonctionner avec un LLM local via une API compatible OpenAI. Cela alimente √† la fois la **g√©n√©ration** et la **critique** (review) des hypoth√®ses.

1.  **D√©marrez votre serveur LLM** :
    *   **LM Studio** : Allez dans l'onglet "Local Server" et d√©marrez le serveur (port 1234 par d√©faut).
    *   **Ollama** : Lancez `ollama serve` (port 11434 par d√©faut).

2.  **Configuration** :
    *   **Via l'interface (recommand√©)** : Entrez simplement l'URL et le nom du mod√®le dans la barre lat√©rale de l'application Streamlit.
        *   URL par d√©faut : `http://127.0.0.1:1234/v1`
        *   Mod√®le par d√©faut : `openai/gpt-oss-20b`
    *   **Via CLI** : Configurez les variables d'environnement :
        ```bash
        export OPENAI_API_BASE="http://127.0.0.1:1234/v1"
        export OPENAI_MODEL_NAME="openai/gpt-oss-20b"
        ```

Si aucun LLM n'est d√©tect√©, le syst√®me basculera automatiquement en mode simul√© pour chaque agent.

## üíª Utilisation en Ligne de Commande (CLI)

Si vous pr√©f√©rez utiliser le script sans interface graphique :

```bash
python co_scientist.py
```

Ce script ex√©cute un cycle de recherche complet sur un cas d'usage pr√©d√©fini (repositionnement de m√©dicaments pour la leuc√©mie) et exporte les r√©sultats dans `co_scientist_results.json`.

### Sortie Attendue

Lorsque le LLM local est connect√©, vous verrez :
```
‚úì Generation Agent initialized with local LLM connection.
‚úì Reflection Agent initialized with local LLM connection.

üìö Running literature search...
‚úì Found 5 relevant papers.

üî¨ Generating 5 initial hypotheses...
...
```

## üîß Personnalisation

### Changer le Mod√®le LLM

Le mod√®le utilis√© est d√©fini dans `GenerationAgent` et `ReflectionAgent`. Par d√©faut, il est r√©gl√© sur `"openai/gpt-oss-20b"`. Vous pouvez le changer pour tout autre mod√®le que vous servez localement.

```python
# In co_scientist.py -> GenerationAgent -> _generate_with_llm
response = await asyncio.to_thread(
    self.llm_client.chat.completions.create,
    model="llama3",  # Change this to your model
    ...
)
```

### D√©sactiver le LLM

Pour forcer le mode de simulation, initialisez `CoScientist` avec `use_local_llm=False`.

```python
# In co_scientist.py -> main()
co_scientist = CoScientist(use_local_llm=False)
```

## üìù Limites et Consid√©rations

- **Qualit√© LLM** : La pertinence des hypoth√®ses et des critiques d√©pend fortement du mod√®le utilis√©.
- **Ranking Agent** : L'agent de classement (`RankingAgent`) fonctionne encore en mode simul√© (calcul de scores heuristiques). L'int√©gration du LLM pour simuler les d√©bats est la prochaine √©tape.
- **Acc√®s aux Donn√©es** : Le syst√®me utilise l'API ArXiv publique. Assurez-vous d'avoir une connexion internet active.

## üéì R√©f√©rences

- **Paper** : "Towards an AI co-scientist" - Google DeepMind (2025)
- **Authors** : Gottweis et al.

---

**Status** : ‚úÖ Fonctionnel (Hybride LLM/Simulation + RAG ArXiv + GUI)
**Derni√®re mise √† jour** : Janvier 2026
**Auteur** : Reproduction du framework co-scientist
